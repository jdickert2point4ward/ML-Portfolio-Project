{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70be15dc-2939-41cd-a315-fd749f0676fe",
   "metadata": {},
   "source": [
    "# Machine Learning Pipeline for Predictive Safety Risk Classifier\n",
    "\n",
    "This notebook modularizes our machine learning workflow into a production-ready scikit-learn Pipeline. The pipeline includes data imputation, scaling, and our tuned XGBoost classifier. This structure ensures reproducibility and ease of deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72d1ab70-a154-4464-9a83-3253392f8263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import xgboost as xgb\n",
    "import joblib\n",
    "\n",
    "# For reproducibility\n",
    "RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5e64e4-f519-46c6-9c93-b70cf92fb4d0",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "\n",
    "We load our engineered datasets generated from the Feature Engineering step. These CSV files already have the necessary features, and we drop any rows missing the target (\"Risk\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb2d949d-e658-45ca-8c72-cc3ce863399d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (24102, 7), Val shape: (5165, 7), Test shape: (5165, 7)\n"
     ]
    }
   ],
   "source": [
    "# Load engineered datasets\n",
    "train_df = pd.read_csv(\"../train_engineered.csv\")\n",
    "val_df = pd.read_csv(\"../val_engineered.csv\")\n",
    "test_df = pd.read_csv(\"../test_engineered.csv\")\n",
    "\n",
    "# Drop rows with missing target values\n",
    "train_df = train_df.dropna(subset=['Risk'])\n",
    "val_df = val_df.dropna(subset=['Risk'])\n",
    "test_df = test_df.dropna(subset=['Risk'])\n",
    "\n",
    "# Split Features and Targets\n",
    "X_train = train_df.drop(columns=['Risk'])\n",
    "y_train = train_df['Risk']\n",
    "X_val = val_df.drop(columns=['Risk'])\n",
    "y_val = val_df['Risk']\n",
    "X_test = test_df.drop(columns=['Risk'])\n",
    "y_test = test_df['Risk']\n",
    "\n",
    "print(f\"Train shape: {X_train.shape}, Val shape: {X_val.shape}, Test shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25290b7d-4bcb-4b72-b748-068855e257f1",
   "metadata": {},
   "source": [
    "## Build the Production Pipeline\n",
    "\n",
    "In this pipeline, we perform:\n",
    "- **Imputation:** Using a simple imputer (constant fill of 0) to handle any residual missing values.\n",
    "- **Scaling:** Standardizing features with `StandardScaler` for robustness.\n",
    "- **Classification:** Using our tuned XGBoost classifier with parameters determined in the ML Workflow.\n",
    "\n",
    "This design ensures that the same preprocessing is applied during both training and inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0060b6a0-080c-4795-9f32-05ac14396a94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline successfully fitted. Pipeline steps:\n",
      "[('imputer', SimpleImputer(fill_value=0, strategy='constant')), ('scaler', StandardScaler()), ('classifier', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='logloss',\n",
      "              feature_types=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.3, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=15,\n",
      "              max_leaves=None, min_child_weight=1, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=500,\n",
      "              n_jobs=None, num_parallel_tree=None, random_state=42, ...))]\n"
     ]
    }
   ],
   "source": [
    "# Build the Pipeline with updated/tuned XGBoost parameters\n",
    "pipeline = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=0)),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"classifier\", xgb.XGBClassifier(\n",
    "        eval_metric=\"logloss\",\n",
    "        random_state=RANDOM_STATE,\n",
    "        scale_pos_weight=10000,  # Tuned to balance the class imbalance.\n",
    "        max_depth=15,\n",
    "        learning_rate=0.3,\n",
    "        min_child_weight=1,\n",
    "        n_estimators=500  # Tuned number of trees\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Fit the pipeline on the training data\n",
    "pipeline.fit(X_train, y_train)\n",
    "print(\"Pipeline successfully fitted. Pipeline steps:\")\n",
    "print(pipeline.steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa50be7-dc94-4290-b973-add15978a897",
   "metadata": {},
   "source": [
    "## Evaluation on the Validation Set\n",
    "\n",
    "We evaluate the pipeline on the validation set to ensure that the preprocessing steps and model predictions generalize well.  \n",
    "*Note:* If needed, you can adjust the prediction threshold outside the pipeline by using `pipeline.predict_proba(X_val)` to obtain probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06367d31-e91c-4c30-ae80-97df55d97908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline (XGBoost) - Validation Set Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      4149\n",
      "           1       1.00      1.00      1.00      1016\n",
      "\n",
      "    accuracy                           1.00      5165\n",
      "   macro avg       1.00      1.00      1.00      5165\n",
      "weighted avg       1.00      1.00      1.00      5165\n",
      "\n",
      "Accuracy: 1.000\n"
     ]
    }
   ],
   "source": [
    "# Validate the pipeline performance on the validation set\n",
    "y_val_pred = pipeline.predict(X_val)\n",
    "print(\"Pipeline (XGBoost) - Validation Set Performance:\")\n",
    "print(classification_report(y_val, y_val_pred))\n",
    "print(f\"Accuracy: {accuracy_score(y_val, y_val_pred):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7538f4-4611-4d2a-9d1d-fc8bd600f65c",
   "metadata": {},
   "source": [
    "## Final Evaluation on the Test Set\n",
    "\n",
    "We now test the final pipeline on the held-out test set to assess its generalization performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "644cd7b8-02c5-4508-8456-c1b828e629f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline (XGBoost) - Test Set Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      4150\n",
      "           1       1.00      1.00      1.00      1015\n",
      "\n",
      "    accuracy                           1.00      5165\n",
      "   macro avg       1.00      1.00      1.00      5165\n",
      "weighted avg       1.00      1.00      1.00      5165\n",
      "\n",
      "Accuracy: 1.000\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the pipeline on the test set\n",
    "y_test_pred = pipeline.predict(X_test)\n",
    "print(\"Pipeline (XGBoost) - Test Set Performance:\")\n",
    "print(classification_report(y_test, y_test_pred))\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_test_pred):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ce4094-ccdb-4890-88ef-efa4065de1ba",
   "metadata": {},
   "source": [
    "## Save the Production Pipeline\n",
    "\n",
    "After verifying performance, we save the entire pipeline. This saved file can later be loaded for inference on new data without needing to re-run the preprocessing or retrain the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d086ebbf-d629-4a5a-85ea-8a0d00fcb6e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline saved as 'safety_risk_pipeline.pk1'\n"
     ]
    }
   ],
   "source": [
    "# Save the Pipeline for future use\n",
    "joblib.dump(pipeline, \"../model/safety_risk_pipeline.pk1\")\n",
    "print(\"Pipeline saved as 'safety_risk_pipeline.pk1'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397ed9d5-38f3-4048-9b41-0a6dc151a1c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
