{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f1402f1-0ac4-4c01-b573-e7ea8ad09b48",
   "metadata": {},
   "source": [
    "# Feature Engineering for Predictive Safety Risk Classifier\n",
    "\n",
    "This notebook performs feature engineering on the transformed Chicago Crime Dataset to prepare it for modeling. We start with a train-test-validation split to avoid data leakage, then engineer features to enhance our safety risk predictions.\n",
    "\n",
    "## Dependencies\n",
    "- `pandas` and `numpy`: For data manipulation.\n",
    "- `scikit-learn`: For splitting the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ee5b4b9e-a312-486d-89b1-0f826b435a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial shape: (34432, 5)\n",
      "    latitude  longitude  CrimeCount  ViolentCount  Risk\n",
      "0  41.644604 -87.610728           1             0     0\n",
      "1  41.644608 -87.598848           1             0     0\n",
      "2  41.645378 -87.540022           1             0     0\n",
      "3  41.646123 -87.542896           1             0     0\n",
      "4  41.647038 -87.616003           1             1     0\n"
     ]
    }
   ],
   "source": [
    "#Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Step 1: Load the transformed object\n",
    "# - File: chicago_crimes_latest_transformed.csv from ETL step\n",
    "df = pd.read_csv(\"chicago_crimes_latest_transformed.csv\") \n",
    "print(f\"Initial shape: {df.shape}\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebace5fd-67b9-4f76-b748-9ccd29eddbde",
   "metadata": {},
   "source": [
    "## Step 1: Train-Test-Validation Split\n",
    "Before feature engineering, we split the data into train (70%), validation (15%), and test (15%) sets to prevent leakage of test information into the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd0f7bde-2c66-4160-a009-d486eab91612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (24102, 4), Validation shape: (5165, 4), Test shape: (5165, 4)\n",
      "Train risk distribution:\n",
      "Risk\n",
      "0    0.803336\n",
      "1    0.196664\n",
      "Name: proportion, dtype: float64\n",
      "Validation risk distribution:\n",
      "Risk\n",
      "0    0.803291\n",
      "1    0.196709\n",
      "Name: proportion, dtype: float64\n",
      "Test risk distribution:\n",
      "Risk\n",
      "0    0.803485\n",
      "1    0.196515\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Train-Test-Validation Split\n",
    "# Step 1: Define features (X) and target (y)\n",
    "X = df.drop(columns=[\"Risk\"]) #Features: latitude, longitude, CrimeCount, ViolentCount\n",
    "y = df[\"Risk\"]\n",
    "\n",
    "# Step 2: First split: 70% train, 30% temp (val + test)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "# Step 3: Second split: 15% validation, 15% test from temp\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n",
    "\n",
    "# Step 4: Verify split sizes\n",
    "print(f\"Train shape: {X_train.shape}, Validation shape: {X_val.shape}, Test shape: {X_test.shape}\")\n",
    "print(f\"Train risk distribution:\\n{y_train.value_counts(normalize=True)}\")\n",
    "print(f\"Validation risk distribution:\\n{y_val.value_counts(normalize=True)}\")\n",
    "print(f\"Test risk distribution:\\n{y_test.value_counts(normalize=True)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036a6b19-47e2-4f3a-a17d-5baa7e68b0c3",
   "metadata": {},
   "source": [
    "## Step 2: Feature Engineering\n",
    "We create new features to improve model performance, applying transformations only to the training set initially, then replicating them on validation and test sets to maintain consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b3bde712-f84d-4134-bbd7-f8708fffecc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set with new features:\n",
      "        latitude  longitude  CrimeCount  ViolentCount  CrimeDensity  \\\n",
      "8215   41.766610 -87.571418           1             0      0.000273   \n",
      "2682   41.719901 -87.563490           2             2      0.000547   \n",
      "20512  41.880992 -87.704487           2             0      0.000544   \n",
      "4882   41.745807 -87.604266           2             1      0.000547   \n",
      "25661  41.909922 -87.723248           1             0      0.000272   \n",
      "\n",
      "       ViolentRatio  DistanceFromCenter  \n",
      "8215            0.0            0.125851  \n",
      "2682            1.0            0.171534  \n",
      "20512           0.0            0.074743  \n",
      "4882            0.5            0.134734  \n",
      "25661           0.0            0.098718  \n",
      "           latitude     longitude    CrimeCount  ViolentCount  CrimeDensity  \\\n",
      "count  24102.000000  24102.000000  24102.000000  24102.000000  24102.000000   \n",
      "mean      41.844080    -87.671596      1.447930      0.451332      0.000395   \n",
      "std        0.087877      0.058380      1.822483      0.765052      0.000497   \n",
      "min       41.644604    -87.927365      1.000000      0.000000      0.000271   \n",
      "25%       41.769401    -87.712852      1.000000      0.000000      0.000272   \n",
      "50%       41.857724    -87.666969      1.000000      0.000000      0.000273   \n",
      "75%       41.910468    -87.629195      1.000000      1.000000      0.000274   \n",
      "max       42.022535    -87.525482     62.000000     17.000000      0.016923   \n",
      "\n",
      "       ViolentRatio  DistanceFromCenter  \n",
      "count  24102.000000        24102.000000  \n",
      "mean       0.310290            0.107944  \n",
      "std        0.438828            0.048813  \n",
      "min        0.000000            0.000358  \n",
      "25%        0.000000            0.075926  \n",
      "50%        0.000000            0.110203  \n",
      "75%        1.000000            0.138391  \n",
      "max        1.000000            0.323917  \n"
     ]
    }
   ],
   "source": [
    "# Feature Engineering\n",
    "# Step 1: Define a function to engineer features\n",
    "def engineer_features(df):\n",
    "    # Feature 1: Crime Density (CrimeCount normalized by unique locations)\n",
    "    df[\"CrimeDensity\"] = df[\"CrimeCount\"] / (df[\"latitude\"] * df[\"longitude\"]).abs()\n",
    "\n",
    "    # Feature 2: Violent Crime Ratio\n",
    "    df[\"ViolentRatio\"] = df[\"ViolentCount\"] / df[\"CrimeCount\"].replace(0, 1) # Avoid division by zero\n",
    "\n",
    "    # Feature 3: Distance from City Center (Chicago: 41.8781 N, -87.6298 W)\n",
    "    city_center_lat, city_center_lon = 41.8781, -87.6298\n",
    "    df[\"DistanceFromCenter\"] = np.sqrt(\n",
    "        (df[\"latitude\"] - city_center_lat)**2 + (df[\"longitude\"] - city_center_lon)**2\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "# Step 2: Apply to train, valudation, and test sets\n",
    "X_train_fe = engineer_features(X_train.copy())\n",
    "X_val_fe = engineer_features(X_val.copy())\n",
    "X_test_fe = engineer_features(X_test.copy())\n",
    "\n",
    "# Step 3: Verify new features\n",
    "print(\"Training set with new features:\")\n",
    "print(X_train_fe.head())\n",
    "print(X_train_fe.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0598fa1-14b0-42b1-97d3-d711a88eadde",
   "metadata": {},
   "source": [
    "## Step 3: Save the Engineered Datasets\n",
    "We recombine features with targets and save the split datasets for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fa380876-ac54-4a58-8a49-a876a1b2d1ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineered datasets saved: train_engineered.csv, val_engineered.csv, test_engineered.csv\n"
     ]
    }
   ],
   "source": [
    "# Save the Engineered Datasets\n",
    "# Step 1: Recombine features and targets\n",
    "train_df = pd.concat([X_train_fe, y_train.reset_index(drop=True)], axis=1)\n",
    "val_df = pd.concat([X_val_fe, y_val.reset_index(drop=True)], axis=1)\n",
    "test_df = pd.concat([X_test_fe, y_test.reset_index(drop=True)], axis=1)\n",
    "\n",
    "# Step 2: Export to CSV\n",
    "train_df.to_csv(\"train_egineered.csv\", index=False)\n",
    "val_df.to_csv(\"val_engineered.csv\", index=False)\n",
    "test_df.to_csv(\"test_engineered.csv\", index=False)\n",
    "\n",
    "# Step 3: Confirm completion\n",
    "print(\"Engineered datasets saved: train_engineered.csv, val_engineered.csv, test_engineered.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d999aed8-dccd-47cc-a5a8-b3464293d173",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
