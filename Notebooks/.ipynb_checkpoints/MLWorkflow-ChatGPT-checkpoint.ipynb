{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f16f14aa-7c4e-4b60-a5d7-c4145c14f206",
   "metadata": {},
   "source": [
    "# Machine Learning Workflow for Predictive Safety Risk Classifier\n",
    "\n",
    "This notebook implements an end-to-end ML workflow for predicting high-risk safety zones in Chicago. We:\n",
    "- Load the engineered datasets (from the Feature Engineering step)\n",
    "- Establish a baseline with Logistic Regression\n",
    "- Iterate over candidate models (Random Forest and XGBoost)\n",
    "- Tune the best-performing model (XGBoost) via GridSearchCV and threshold selection\n",
    "- Evaluate on a held-out test set and save the final model for deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8834771-a769-4b4e-9550-3233e76e7bb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced y_train distribution:\n",
      "Risk\n",
      "0    0.5\n",
      "1    0.5\n",
      "Name: proportion, dtype: float64\n",
      "Train shape: (38724, 6), Val shape: (5165, 6), Test shape: (5165, 6)\n"
     ]
    }
   ],
   "source": [
    "# Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import xgboost as xgb\n",
    "import joblib\n",
    "\n",
    "# Step 1: Load engineered datasets\n",
    "train_df = pd.read_csv(\"../train_engineered.csv\").dropna()\n",
    "val_df = pd.read_csv(\"../val_engineered.csv\").dropna()\n",
    "test_df = pd.read_csv(\"../test_engineered.csv\").dropna()\n",
    "\n",
    "# Step 2: Split features and target (dropping 'CrimeDensity' since it is an engineered feature we won't use here)\n",
    "X_train = train_df.drop(columns=[\"Risk\", \"CrimeDensity\"])\n",
    "y_train = train_df[\"Risk\"]\n",
    "X_val = val_df.drop(columns=[\"Risk\", \"CrimeDensity\"])\n",
    "y_val = val_df[\"Risk\"]\n",
    "X_test = test_df.drop(columns=[\"Risk\", \"CrimeDensity\"])\n",
    "y_test = test_df[\"Risk\"]\n",
    "\n",
    "# Balance the training set using SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_bal, y_train_bal = smote.fit_resample(X_train, y_train)\n",
    "print(\"Balanced y_train distribution:\")\n",
    "print(y_train_bal.value_counts(normalize=True))\n",
    "print(f\"Train shape: {X_train_bal.shape}, Val shape: {X_val.shape}, Test shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2774cf74-5ea0-41d8-8124-dfd66f87a982",
   "metadata": {},
   "source": [
    "## Step 1: Baseline Model\n",
    "\n",
    "We begin with a simple Logistic Regression model as our baseline. Note the extreme class weight used to force the model to learn from the minority class. We then examine the probability distribution and set a low threshold to capture nearly all positives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96276f83-3f77-4312-ba0f-6b6af19efadb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Logistic Regression on balanced data with 38724 rows.\n",
      "Balanced y_train distribution:\n",
      " Risk\n",
      "0    0.5\n",
      "1    0.5\n",
      "Name: proportion, dtype: float64\n",
      "Logistic Regression probability range: 0.9999792692244465 - 0.9999933979059933\n",
      "First 20 predicted probabilities: [0.99997943 0.99997943 0.9999797  0.99997962 0.99998026 0.99998708\n",
      " 0.99997955 0.99997976 0.99997998 0.99997968 0.99998004 0.99997958\n",
      " 0.99997971 0.99997972 0.99997984 0.99997948 0.99997998 0.99998006\n",
      " 0.99997953 0.99997958]\n",
      "\n",
      "Logistic Regression - Validation Set Performance (threshold 0.000005):\n",
      "Predicted distribution:\n",
      " 1    1.0\n",
      "Name: proportion, dtype: float64\n",
      "Confusion Matrix:\n",
      " [[   0 4149]\n",
      " [   0 1016]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00      4149\n",
      "           1       0.20      1.00      0.33      1016\n",
      "\n",
      "    accuracy                           0.20      5165\n",
      "   macro avg       0.10      0.50      0.16      5165\n",
      "weighted avg       0.04      0.20      0.06      5165\n",
      "\n",
      "Accuracy: 0.197\n"
     ]
    }
   ],
   "source": [
    "# Baseline: Logistic Regression\n",
    "lr_model = LogisticRegression(max_iter=1000, random_state=42, C=0.1, class_weight={0: 1, 1: 50000})\n",
    "print(\"Training Logistic Regression on balanced data with\", X_train_bal.shape[0], \"rows.\")\n",
    "print(\"Balanced y_train distribution:\\n\", pd.Series(y_train_bal).value_counts(normalize=True))\n",
    "\n",
    "lr_model.fit(X_train_bal, y_train_bal)\n",
    "y_val_prob_lr = lr_model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "print(\"Logistic Regression probability range:\", y_val_prob_lr.min(), \"-\", y_val_prob_lr.max())\n",
    "print(\"First 20 predicted probabilities:\", y_val_prob_lr[:20])\n",
    "\n",
    "# Using an extremely low threshold to force positive predictions (for baseline comparison)\n",
    "baseline_thresh = 0.000005\n",
    "y_val_pred_lr = (y_val_prob_lr >= baseline_thresh).astype(int)\n",
    "\n",
    "print(\"\\nLogistic Regression - Validation Set Performance (threshold {:.6f}):\".format(baseline_thresh))\n",
    "print(\"Predicted distribution:\\n\", pd.Series(y_val_pred_lr).value_counts(normalize=True))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_val, y_val_pred_lr))\n",
    "print(classification_report(y_val, y_val_pred_lr, zero_division=0))\n",
    "print(f\"Accuracy: {accuracy_score(y_val, y_val_pred_lr):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30677542-68d0-49ab-a00f-bd6989227706",
   "metadata": {},
   "source": [
    "## Step 2: Model Iteration\n",
    "\n",
    "We now test two candidate models: Random Forest and XGBoost. Our goal is to compare performance (in terms of recall, accuracy, and predicted positives) against the baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0f2f4c1-abe2-4564-baec-45411d3368dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Random Forest...\n",
      "Random Forest probability range: 0.0 - 1.0\n",
      "\n",
      "Random Forest - Validation Set Performance (threshold 0.00005):\n",
      "Predicted distribution:\n",
      " 0    0.801355\n",
      "1    0.198645\n",
      "Name: proportion, dtype: float64\n",
      "Confusion Matrix:\n",
      " [[4139   10]\n",
      " [   0 1016]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      4149\n",
      "           1       0.99      1.00      1.00      1016\n",
      "\n",
      "    accuracy                           1.00      5165\n",
      "   macro avg       1.00      1.00      1.00      5165\n",
      "weighted avg       1.00      1.00      1.00      5165\n",
      "\n",
      "Accuracy: 0.998\n",
      "\n",
      "Training XGBoost...\n",
      "XGBoost probability range: 4.4638273e-05 - 1.0\n",
      "\n",
      "XGBoost - Validation Set Performance (threshold 0.005):\n",
      "Predicted distribution:\n",
      " 0    0.803291\n",
      "1    0.196709\n",
      "Name: proportion, dtype: float64\n",
      "Confusion Matrix:\n",
      " [[4149    0]\n",
      " [   0 1016]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      4149\n",
      "           1       1.00      1.00      1.00      1016\n",
      "\n",
      "    accuracy                           1.00      5165\n",
      "   macro avg       1.00      1.00      1.00      5165\n",
      "weighted avg       1.00      1.00      1.00      5165\n",
      "\n",
      "Accuracy: 1.000\n"
     ]
    }
   ],
   "source": [
    "# Candidate Model 1: Random Forest\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=500,\n",
    "    max_depth=None,\n",
    "    random_state=42,\n",
    "    class_weight={0: 1, 1: 100000}\n",
    ")\n",
    "print(\"Training Random Forest...\")\n",
    "rf_model.fit(X_train_bal, y_train_bal)\n",
    "y_val_prob_rf = rf_model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "print(\"Random Forest probability range:\", y_val_prob_rf.min(), \"-\", y_val_prob_rf.max())\n",
    "y_val_pred_rf = (y_val_prob_rf >= 0.00005).astype(int)\n",
    "print(\"\\nRandom Forest - Validation Set Performance (threshold 0.00005):\")\n",
    "print(\"Predicted distribution:\\n\", pd.Series(y_val_pred_rf).value_counts(normalize=True))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_val, y_val_pred_rf))\n",
    "print(classification_report(y_val, y_val_pred_rf, zero_division=0))\n",
    "print(f\"Accuracy: {accuracy_score(y_val, y_val_pred_rf):.3f}\")\n",
    "\n",
    "# Candidate Model 2: XGBoost\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    eval_metric=\"logloss\",\n",
    "    random_state=42,\n",
    "    scale_pos_weight=10000,\n",
    "    max_depth=15,\n",
    "    learning_rate=0.3,\n",
    "    min_child_weight=1\n",
    ")\n",
    "print(\"\\nTraining XGBoost...\")\n",
    "xgb_model.fit(X_train_bal, y_train_bal)\n",
    "y_val_prob_xgb = xgb_model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "print(\"XGBoost probability range:\", y_val_prob_xgb.min(), \"-\", y_val_prob_xgb.max())\n",
    "y_val_pred_xgb = (y_val_prob_xgb >= 0.005).astype(int)\n",
    "print(\"\\nXGBoost - Validation Set Performance (threshold 0.005):\")\n",
    "print(\"Predicted distribution:\\n\", pd.Series(y_val_pred_xgb).value_counts(normalize=True))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_val, y_val_pred_xgb))\n",
    "print(classification_report(y_val, y_val_pred_xgb, zero_division=0))\n",
    "print(f\"Accuracy: {accuracy_score(y_val, y_val_pred_xgb):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107931a1-ebb0-41f0-aecd-784368744709",
   "metadata": {},
   "source": [
    "## Step 3: Hyperparameter Tuning of XGBoost\n",
    "\n",
    "Based on the iteration results, XGBoost shows a promising balance of recall and accuracy. We now tune its hyperparameters using GridSearchCV on the balanced and scaled training data. We also perform a threshold sweep on the validation set to fine-tune the predicted positives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "550c32b9-320a-41d9-81ef-b297c1e9e6f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 108 candidates, totalling 324 fits\n",
      "Best XGBoost parameters: {'learning_rate': 0.1, 'max_depth': 10, 'min_child_weight': 1, 'n_estimators': 100, 'scale_pos_weight': 10000}\n",
      "\n",
      "Threshold Tuning on Validation Set:\n",
      "\n",
      "Threshold: 0.001\n",
      "Predicted positives: 1016 out of 5165\n",
      "Confusion Matrix:\n",
      " [[4149    0]\n",
      " [   0 1016]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      4149\n",
      "           1       1.00      1.00      1.00      1016\n",
      "\n",
      "    accuracy                           1.00      5165\n",
      "   macro avg       1.00      1.00      1.00      5165\n",
      "weighted avg       1.00      1.00      1.00      5165\n",
      "\n",
      "Accuracy: 1.000\n",
      "\n",
      "Threshold: 0.002\n",
      "Predicted positives: 1016 out of 5165\n",
      "Confusion Matrix:\n",
      " [[4149    0]\n",
      " [   0 1016]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      4149\n",
      "           1       1.00      1.00      1.00      1016\n",
      "\n",
      "    accuracy                           1.00      5165\n",
      "   macro avg       1.00      1.00      1.00      5165\n",
      "weighted avg       1.00      1.00      1.00      5165\n",
      "\n",
      "Accuracy: 1.000\n",
      "\n",
      "Threshold: 0.003\n",
      "Predicted positives: 1016 out of 5165\n",
      "Confusion Matrix:\n",
      " [[4149    0]\n",
      " [   0 1016]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      4149\n",
      "           1       1.00      1.00      1.00      1016\n",
      "\n",
      "    accuracy                           1.00      5165\n",
      "   macro avg       1.00      1.00      1.00      5165\n",
      "weighted avg       1.00      1.00      1.00      5165\n",
      "\n",
      "Accuracy: 1.000\n",
      "\n",
      "Threshold: 0.004\n",
      "Predicted positives: 1016 out of 5165\n",
      "Confusion Matrix:\n",
      " [[4149    0]\n",
      " [   0 1016]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      4149\n",
      "           1       1.00      1.00      1.00      1016\n",
      "\n",
      "    accuracy                           1.00      5165\n",
      "   macro avg       1.00      1.00      1.00      5165\n",
      "weighted avg       1.00      1.00      1.00      5165\n",
      "\n",
      "Accuracy: 1.000\n",
      "\n",
      "Threshold: 0.005\n",
      "Predicted positives: 1016 out of 5165\n",
      "Confusion Matrix:\n",
      " [[4149    0]\n",
      " [   0 1016]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      4149\n",
      "           1       1.00      1.00      1.00      1016\n",
      "\n",
      "    accuracy                           1.00      5165\n",
      "   macro avg       1.00      1.00      1.00      5165\n",
      "weighted avg       1.00      1.00      1.00      5165\n",
      "\n",
      "Accuracy: 1.000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Scale the training, validation, and test sets\n",
    "scaler = StandardScaler()\n",
    "X_train_bal_scaled = scaler.fit_transform(X_train_bal)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Define parameter grid for XGBoost\n",
    "param_grid = {\n",
    "    'max_depth': [10, 15, 20],\n",
    "    'learning_rate': [0.1, 0.3, 0.5],\n",
    "    'n_estimators': [100, 500, 1000],\n",
    "    'min_child_weight': [1, 3],\n",
    "    'scale_pos_weight': [10000, 15000]\n",
    "}\n",
    "\n",
    "xgb_estimator = xgb.XGBClassifier(eval_metric=\"logloss\", random_state=42)\n",
    "\n",
    "grid_search = GridSearchCV(estimator=xgb_estimator, param_grid=param_grid, scoring='accuracy', cv=3, n_jobs=-1, verbose=1)\n",
    "grid_search.fit(X_train_bal_scaled, y_train_bal)\n",
    "\n",
    "print(\"Best XGBoost parameters:\", grid_search.best_params_)\n",
    "best_xgb = grid_search.best_estimator_\n",
    "\n",
    "# Perform threshold tuning on the validation set\n",
    "y_val_prob_xgb_tuned = best_xgb.predict_proba(X_val_scaled)[:, 1]\n",
    "thresholds = [0.001, 0.002, 0.003, 0.004, 0.005]\n",
    "\n",
    "print(\"\\nThreshold Tuning on Validation Set:\")\n",
    "for thresh in thresholds:\n",
    "    y_val_pred = (y_val_prob_xgb_tuned >= thresh).astype(int)\n",
    "    pos_count = y_val_pred.sum()\n",
    "    print(f\"\\nThreshold: {thresh}\")\n",
    "    print(\"Predicted positives:\", pos_count, \"out of\", len(y_val_pred))\n",
    "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_val, y_val_pred))\n",
    "    print(classification_report(y_val, y_val_pred, zero_division=0))\n",
    "    print(f\"Accuracy: {accuracy_score(y_val, y_val_pred):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1683fcb4-29cf-436e-aa6d-0f1a96475e7d",
   "metadata": {},
   "source": [
    "## Step 4: Final Evaluation on the Test Set\n",
    "\n",
    "Using the tuned XGBoost model (and an appropriate threshold determined from the validation set, e.g., 0.003), we evaluate the final model on the held-out test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5e79d53-9e76-4f68-b3d9-d60593253d68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final XGBoost - Test Set Performance (threshold 0.003):\n",
      "Predicted distribution:\n",
      " 0    0.803485\n",
      "1    0.196515\n",
      "Name: proportion, dtype: float64\n",
      "Confusion Matrix:\n",
      " [[4150    0]\n",
      " [   0 1015]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      4150\n",
      "           1       1.00      1.00      1.00      1015\n",
      "\n",
      "    accuracy                           1.00      5165\n",
      "   macro avg       1.00      1.00      1.00      5165\n",
      "weighted avg       1.00      1.00      1.00      5165\n",
      "\n",
      "Test Set Accuracy: 1.000\n"
     ]
    }
   ],
   "source": [
    "# Select the best threshold based on validation (adjust as needed)\n",
    "best_thresh = 0.003\n",
    "\n",
    "y_test_prob = best_xgb.predict_proba(X_test_scaled)[:, 1]\n",
    "y_test_pred = (y_test_prob >= best_thresh).astype(int)\n",
    "\n",
    "print(\"Final XGBoost - Test Set Performance (threshold {:.3f}):\".format(best_thresh))\n",
    "print(\"Predicted distribution:\\n\", pd.Series(y_test_pred).value_counts(normalize=True))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_test_pred))\n",
    "print(classification_report(y_test, y_test_pred, zero_division=0))\n",
    "print(f\"Test Set Accuracy: {accuracy_score(y_test, y_test_pred):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103cbf43-70b2-422f-a788-34142e66f24a",
   "metadata": {},
   "source": [
    "## Step 5: Save the Best Model\n",
    "\n",
    "We now save the tuned XGBoost model (along with the scaler) for future use or deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71fca25d-88d5-43ee-965a-6c529de4feef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best XGBoost model saved as 'best_model.pk1'\n",
      "Scaler saved as 'scaler.pkl'\n"
     ]
    }
   ],
   "source": [
    "# Save the best XGBoost model and the scaler for inference\n",
    "joblib.dump(best_xgb, \"../best_model.pk1\")\n",
    "joblib.dump(scaler, \"../scaler.pkl\")\n",
    "print(\"Best XGBoost model saved as 'best_model.pk1'\")\n",
    "print(\"Scaler saved as 'scaler.pkl'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bde32f-6b3a-4f74-ac70-1107dd88cf3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
