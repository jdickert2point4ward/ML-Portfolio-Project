{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f1402f1-0ac4-4c01-b573-e7ea8ad09b48",
   "metadata": {},
   "source": [
    "# Feature Engineering for Predictive Safety Risk Classifier\n",
    "\n",
    "This notebook performs feature engineering on the transformed Chicago Crime Dataset to prepare it for modeling. We start with a train–test–validation split to avoid data leakage, then create new features that improve our safety risk predictions.\n",
    "\n",
    "**Overview:**\n",
    "- **Dependencies:** pandas, numpy, and scikit-learn.\n",
    "- **Steps:**\n",
    "  1. Load the transformed dataset.\n",
    "  2. Split the data into train (70%), validation (15%), and test (15%) sets.\n",
    "  3. Engineer new features on the training set and replicate these transformations on the validation and test sets.\n",
    "  4. Save the engineered datasets for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee5b4b9e-a312-486d-89b1-0f826b435a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial dataset shape: (34432, 5)\n",
      "    latitude  longitude  CrimeCount  ViolentCount  Risk\n",
      "0  41.644604 -87.610728           1             0     0\n",
      "1  41.644608 -87.598848           1             0     0\n",
      "2  41.645378 -87.540022           1             0     0\n",
      "3  41.646123 -87.542896           1             0     0\n",
      "4  41.647038 -87.616003           1             1     0\n"
     ]
    }
   ],
   "source": [
    "# Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Step 1: Load the transformed dataset\n",
    "# File generated in the ETL step\n",
    "df = pd.read_csv(\"chicago_crimes_latest_transformed.csv\")\n",
    "print(f\"Initial dataset shape: {df.shape}\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebace5fd-67b9-4f76-b748-9ccd29eddbde",
   "metadata": {},
   "source": [
    "## Step 1: Train–Test–Validation Split\n",
    "\n",
    "Before applying feature engineering, we split the data into training, validation, and test sets to prevent any leakage of test information into our training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd0f7bde-2c66-4160-a009-d486eab91612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaNs in target before split: 0\n",
      "Training set shape: (24102, 4)\n",
      "Validation set shape: (5165, 4)\n",
      "Test set shape: (5165, 4)\n",
      "\n",
      "Training risk distribution:\n",
      " Risk\n",
      "0    0.803336\n",
      "1    0.196664\n",
      "Name: proportion, dtype: float64\n",
      "Validation risk distribution:\n",
      " Risk\n",
      "0    0.803291\n",
      "1    0.196709\n",
      "Name: proportion, dtype: float64\n",
      "Test risk distribution:\n",
      " Risk\n",
      "0    0.803485\n",
      "1    0.196515\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Define features (X) and target (y)\n",
    "X = df.drop(columns=[\"Risk\"])  # Features: latitude, longitude, CrimeCount, ViolentCount\n",
    "y = df[\"Risk\"]\n",
    "\n",
    "# Ensure that only rows with non-null target values are retained\n",
    "print(\"NaNs in target before split:\", y.isnull().sum())\n",
    "X = X[y.notnull()]\n",
    "y = y[y.notnull()]\n",
    "\n",
    "# Step 2: Split data into train (70%) and temporary set (30%: for validation and test)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Step 3: Split temporary set into validation (15%) and test (15%)\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "# Verify split sizes and target distribution\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Validation set shape: {X_val.shape}\")\n",
    "print(f\"Test set shape: {X_test.shape}\\n\")\n",
    "print(\"Training risk distribution:\\n\", y_train.value_counts(normalize=True))\n",
    "print(\"Validation risk distribution:\\n\", y_val.value_counts(normalize=True))\n",
    "print(\"Test risk distribution:\\n\", y_test.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036a6b19-47e2-4f3a-a17d-5baa7e68b0c3",
   "metadata": {},
   "source": [
    "## Step 2: Feature Engineering\n",
    "\n",
    "We now create new features that can help improve model performance. All transformations are first developed on the training set and then applied to the validation and test sets for consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3bde712-f84d-4134-bbd7-f8708fffecc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set with new features:\n",
      "        latitude  longitude  CrimeCount  ViolentCount  CrimeDensity  \\\n",
      "8215   41.766610 -87.571418           1             0      0.000273   \n",
      "2682   41.719901 -87.563490           2             2      0.000547   \n",
      "20512  41.880992 -87.704487           2             0      0.000544   \n",
      "4882   41.745807 -87.604266           2             1      0.000547   \n",
      "25661  41.909922 -87.723248           1             0      0.000272   \n",
      "\n",
      "       ViolentRatio  DistanceFromCenter  \n",
      "8215            0.0            0.125851  \n",
      "2682            1.0            0.171534  \n",
      "20512           0.0            0.074743  \n",
      "4882            0.5            0.134734  \n",
      "25661           0.0            0.098718  \n",
      "\n",
      "Statistical summary of engineered features:\n",
      "           latitude     longitude    CrimeCount  ViolentCount  CrimeDensity  \\\n",
      "count  24102.000000  24102.000000  24102.000000  24102.000000  24102.000000   \n",
      "mean      41.844080    -87.671596      1.447930      0.451332      0.000395   \n",
      "std        0.087877      0.058380      1.822483      0.765052      0.000497   \n",
      "min       41.644604    -87.927365      1.000000      0.000000      0.000271   \n",
      "25%       41.769401    -87.712852      1.000000      0.000000      0.000272   \n",
      "50%       41.857724    -87.666969      1.000000      0.000000      0.000273   \n",
      "75%       41.910468    -87.629195      1.000000      1.000000      0.000274   \n",
      "max       42.022535    -87.525482     62.000000     17.000000      0.016923   \n",
      "\n",
      "       ViolentRatio  DistanceFromCenter  \n",
      "count  24102.000000        24102.000000  \n",
      "mean       0.310290            0.107944  \n",
      "std        0.438828            0.048813  \n",
      "min        0.000000            0.000358  \n",
      "25%        0.000000            0.075926  \n",
      "50%        0.000000            0.110203  \n",
      "75%        1.000000            0.138391  \n",
      "max        1.000000            0.323917  \n"
     ]
    }
   ],
   "source": [
    "def engineer_features(df):\n",
    "    \"\"\"\n",
    "    Engineer new features for the safety risk classifier.\n",
    "\n",
    "    Features added:\n",
    "    - CrimeDensity: CrimeCount normalized by a combination of latitude and longitude (avoiding division by zero).\n",
    "    - ViolentRatio: Ratio of violent crimes to total crimes (with zero division handling).\n",
    "    - DistanceFromCenter: Euclidean distance from Chicago city center (41.8781, -87.6298).\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): DataFrame containing at least the columns 'latitude', 'longitude', \n",
    "                           'CrimeCount', and 'ViolentCount'.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with new engineered features.\n",
    "    \"\"\"\n",
    "    # Feature 1: Crime Density\n",
    "    # Avoid division by zero by replacing any zero denominators with 1.\n",
    "    denominator = (df[\"latitude\"] * df[\"longitude\"]).abs()\n",
    "    df[\"CrimeDensity\"] = df[\"CrimeCount\"] / denominator.replace(0, 1)\n",
    "    df[\"CrimeDensity\"] = df[\"CrimeDensity\"].fillna(0)\n",
    "\n",
    "    # Feature 2: Violent Crime Ratio\n",
    "    df[\"ViolentRatio\"] = df[\"ViolentCount\"] / df[\"CrimeCount\"].replace(0, 1)\n",
    "    df[\"ViolentRatio\"] = df[\"ViolentRatio\"].fillna(0)\n",
    "\n",
    "    # Feature 3: Distance from City Center (Chicago: 41.8781 N, -87.6298 W)\n",
    "    city_center_lat, city_center_lon = 41.8781, -87.6298\n",
    "    df[\"DistanceFromCenter\"] = np.sqrt(\n",
    "        (df[\"latitude\"].fillna(city_center_lat) - city_center_lat) ** 2 +\n",
    "        (df[\"longitude\"].fillna(city_center_lon) - city_center_lon) ** 2\n",
    "    )\n",
    "    return df\n",
    "\n",
    "# Apply feature engineering to train, validation, and test sets\n",
    "X_train_fe = engineer_features(X_train.copy())\n",
    "X_val_fe = engineer_features(X_val.copy())\n",
    "X_test_fe = engineer_features(X_test.copy())\n",
    "\n",
    "# Verify that new features have been added\n",
    "print(\"Training set with new features:\")\n",
    "print(X_train_fe.head())\n",
    "print(\"\\nStatistical summary of engineered features:\")\n",
    "print(X_train_fe.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0598fa1-14b0-42b1-97d3-d711a88eadde",
   "metadata": {},
   "source": [
    "## Step 3: Save the Engineered Datasets\n",
    "\n",
    "After engineering the features, we recombine the features with their respective targets and save the datasets. These files will be used in the modeling step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa380876-ac54-4a58-8a49-a876a1b2d1ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN counts in validation set before saving:\n",
      "latitude              0\n",
      "longitude             0\n",
      "CrimeCount            0\n",
      "ViolentCount          0\n",
      "CrimeDensity          0\n",
      "ViolentRatio          0\n",
      "DistanceFromCenter    0\n",
      "Risk                  0\n",
      "dtype: int64\n",
      "Engineered datasets saved successfully as:\n",
      "- train_engineered.csv\n",
      "- val_engineered.csv\n",
      "- test_engineered.csv\n"
     ]
    }
   ],
   "source": [
    "# Recombine features and targets for each split\n",
    "train_df = pd.concat([X_train_fe.reset_index(drop=True), y_train.reset_index(drop=True)], axis=1)\n",
    "val_df = pd.concat([X_val_fe.reset_index(drop=True), y_val.reset_index(drop=True)], axis=1)\n",
    "test_df = pd.concat([X_test_fe.reset_index(drop=True), y_test.reset_index(drop=True)], axis=1)\n",
    "\n",
    "# Check for any remaining NaNs in the validation set (as an example)\n",
    "print(\"NaN counts in validation set before saving:\")\n",
    "print(val_df.isnull().sum())\n",
    "\n",
    "# Export the engineered datasets to CSV\n",
    "train_df.to_csv(\"../train_engineered.csv\", index=False)\n",
    "val_df.to_csv(\"../val_engineered.csv\", index=False)\n",
    "test_df.to_csv(\"../test_engineered.csv\", index=False)\n",
    "\n",
    "print(\"Engineered datasets saved successfully as:\")\n",
    "print(\"- train_engineered.csv\")\n",
    "print(\"- val_engineered.csv\")\n",
    "print(\"- test_engineered.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d999aed8-dccd-47cc-a5a8-b3464293d173",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
